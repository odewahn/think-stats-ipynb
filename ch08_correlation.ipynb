{
  "metadata": {
    "name": "Correlation"
  },
  "nbformat": 3,
  "nbformat_minor": 0,
  "worksheets": [
    {
      "cells": [
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Correlation"
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Standard Scores"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In this chapter, we look at relationships between variables. For example, we have a sense that height is related to weight; people who are taller tend to be heavier. _Correlation_ is a description of this kind of relationship."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "A challenge in measuring correlation is that the variables we want to compare might not be expressed in the same units. For example, height might be in centimeters and weight in kilograms. And even if they are in the same units, they come from different distributions."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "There are two common solutions to these problems:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ol>\n<li>\n<p>Transform all values to <em>standard scores</em>. This leads to the Pearson coefficient of correlation.</p>\n</li>\n<li>\n<p>Transform all values to their percentile ranks. This leads to the Spearman coefficient.</p>\n</li>\n\n</ol>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If _X_ is a series of values, _x_ _i_, we can convert to standard scores by subtracting the mean and dividing by the standard deviation: z\\_i\\_ = (x\\_i\\_ − _μ_) / _σ_."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The numerator is a deviation: the distance from the mean. Dividing by _σ_ _normalizes_ the deviation, so the values of _Z_ are dimensionless (no units) and their distribution has mean 0 and variance 1."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If _X_ is normally distributed, so is _Z_; but if _X_ is skewed or has outliers, so does _Z_. In those cases, it is more robust to use percentile ranks. If _R_ contains the percentile ranks of the values in _X_, the distribution of _R_ is uniform between 0 and 100, regardless of the distribution of _X_."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Covariance"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "_Covariance_ is a measure of the tendency of two variables to vary together. If we have two series, _X_ and _Y_, their deviations from the mean are"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "_d_ _x_ _i_= _x_ _i_ − _μ_ _X_"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "_d_ _y_ _i_= _y_ _i_ − _μ_ _Y_"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "where _μ_ _X_ is the mean of _X_ and _μ_ _Y_ is the mean of _Y_. If _X_ and _Y_ vary together, their deviations tend to have the same sign."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If we multiply them together, the product is positive when the deviations have the same sign and negative when they have the opposite sign. So adding up the products gives a measure of the tendency to vary together."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Covariance is the mean of these products:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "where _n_ is the length of the two series (they have to be the same length)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Covariance is useful in some computations, but it is seldom reported as a summary statistic because it is hard to interpret. Among other problems, its units are the product of the units of _X_ and _Y_. So the covariance of weight and height might be in units of kilogram-meters, which doesn’t mean much."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Making Scatterplots in Pyplot"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The simplest way to check for a relationship between two variables is a scatterplot, but making a good scatterplot is not always easy. As an example, I’ll plot weight versus height for the respondents in the BRFSS (see ???). `pyplot` provides a function named `scatter` that makes scatterplots:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "import matplotlib.pyplot as pyplot\npyplot.scatter(heights, weights)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Figure 8-2 shows the result. Not surprisingly, it looks like there is a positive correlation: taller people tend to be heavier. But this is not the best representation of the data, because the data is packed into columns. The problem is that the heights were rounded to the nearest inch, converted to centimeters, and then rounded again. Some information is lost in translation."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"scatterplot1\"><img src=\"files/figs/web/thst_0902.png\" alt=\"thst 0902\"><figcaption>Simple scatterplot of weight versus height for the respondents in the BRFSS</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We can’t get that information back, but we can minimize the effect on the scatterplot by _jittering_ the data, which means adding random noise to reverse the effect of rounding off. Since these measurements were rounded to the nearest inch, they can be off by up to 0.5 inches or 1.3 cm. So I added uniform noise in the range −1.3 to 1.3:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "jitter = 1.3\nheights = [h + random.uniform(-jitter, jitter) for h in heights]",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Figure 8-3 shows the result. Jittering the data makes the shape of the relationship clearer. In general, you should only jitter data for purposes of visualization and avoid using jittered data for analysis."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"scatterplot2\"><img src=\"files/figs/web/thst_0903.png\" alt=\"thst 0903\"><figcaption>Scatterplot with jittered data</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Even with jittering, this is not the best way to represent the data. There are many overlapping points, which hides data in the dense parts of the figure and gives disproportionate emphasis to outliers."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We can solve that with the `alpha` parameter, which makes the points partly transparent:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "pyplot.scatter(heights, weights, alpha=0.2)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Figure 8-4 shows the result. Overlapping data points look darker, so darkness is proportional to density. In this version of the plot, we can see an apparent artifact: a horizontal line near 90 kg or 200 pounds. Since this data is based on self-reports in pounds, the most likely explanation is some responses were rounded off (possibly down)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"scatterplot3\"><img src=\"files/figs/web/thst_0904.png\" alt=\"thst 0904\"><figcaption>Scatterplot with jittering and transparency</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Using transparency works well for moderate-sized datasets, but this figure only shows the first 1,000 records in the BRFSS, out of a total of 414,509."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To handle larger datasets, one option is a hexbin plot, which divides the graph into hexagonal bins and colors each bin according to how many data points fall in it. `pyplot` provides a function called `hexbin` :"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "pyplot.hexbin(heights, weights, cmap=matplotlib.cm.Blues)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Figure 8-5 shows the result with a blue colormap. An advantage of a hexbin is that it shows the shape of the relationship well, and it is efficient for large datasets. A drawback is that it makes the outliers invisible."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"scatterplot4\"><img src=\"files/figs/web/thst_0905.png\" alt=\"thst 0905\"><figcaption>Scatterplot with binned data using pyplot.hexbin</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The moral of this story is that it is not easy to make a scatterplot that is not potentially misleading. You can download the code for these figures from [http://thinkstats.com/brfss\\_scatter.py](http://thinkstats.com/brfss_scatter.py)."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Spearman’s Rank Correlation"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Pearson’s correlation works well if the relationship between variables is linear and if the variables are roughly normal. But it is not robust in the presence of outliers."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Anscombe’s quartet demonstrates this effect; it contains four datasets with the same correlation. One is a linear relation with random noise, one is a non-linear relation, one is a perfect relation with an outlier, and one has no relation except an artifact caused by an outlier. You can read more about it at [http://wikipedia.org/wiki/Anscombe’s\\_quartet](http://wikipedia.org/wiki/Anscombe%E2%80%99s_quartet)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Spearman’s rank correlation is an alternative that mitigates the effect of outliers and skewed distributions. To compute Spearman’s correction, we have to compute the _rank_ of each value, which is its index in the sorted sample. For example, in the sample {7, 1, 2, 5} the rank of the value 5 is 3, because it appears third if we sort the elements. Then we compute Pearson’s correlation for the ranks."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "An alternative to Spearman’s is to apply a transform that makes the data more nearly normal, the compute Pearson’s correlation for the transformed data. For example, if the data is approximately lognormal, you could take the log of each value and compute the correlation of the logs."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Least Squares Fit"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Correlation coefficients measure the strength and sign of a relationship, but not the slope. There are several ways to estimate the slope; the most common is a _linear least squares fit_. A “linear fit” is a line intended to model the relationship between variables. A “least squares” fit is one that minimizes the mean squared error (MSE) between the line and the data. [17](ch08.html#idp1259632)"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Suppose we have a sequence of points, _Y_, that we want to express as a function of another sequence _X_. If there is a linear relationship between _X_ and _Y_ with intercept _α_ and slope _β_, we expect each _y_ _i_ to be roughly _α_ + _β_ _x_ _i_."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "But unless the correlation is perfect, this prediction is only approximate. The deviation, or _residual_, is"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "_ε_ _i_ = (_α_ + _β_ _x_ _i_) − _y_ _i_"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The residual might be due to random factors like measurement error, or non-random factors that are unknown. For example, if we are trying to predict weight as a function of height, unknown factors might include diet, exercise, and body type."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If we get the parameters _α_ and _β_ wrong, the residuals get bigger, so it makes intuitive sense that the parameters we want are the ones that minimize the residuals."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "As usual, we could minimize the absolute value of the residuals, or their squares, or their cubes, etc. The most common choice is to minimize the sum of squared residuals."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Why? There are three good reasons and one bad one:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li>\n<p>Squaring has the obvious feature of treating positive and negative residuals the same, which is usually what we want.</p>\n</li>\n<li>\n<p>Squaring gives more weight to large residuals, but not so much weight that the largest residual always dominates.</p>\n</li>\n<li>\n<p>If the residuals are independent of <em>x</em>, random, and normally distributed with <em>μ</em> = 0 and constant (but unknown) <em>σ</em>, then the least squares fit is also the maximum likelihood estimator of <em>α</em> and <em>β</em>.<a data-type=\"noteref\" id=\"idp1280576-marker\" href=\"ch08.html#idp1280576\"><sup>18</sup></a></p>\n</li>\n<li>\n<p>The values of <img src=\"figs/web/equations/img-0075.png\" alt=\"img-0075\"> and <img src=\"figs/web/equations/img-0076.png\" alt=\"img-0076\"> that minimize the squared residuals can be computed efficiently.</p>\n</li>\n</ul>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "That last reason made sense when computational efficiency was more important than choosing the method most appropriate to the problem at hand. That’s no longer the case, so it is worth considering whether squared residuals are the right thing to minimize."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For example, if you are using values of _X_ to predict values of _Y_, guessing too high might be better (or worse) than guessing too low. In that case, you might want to compute some cost function, cost(_ε_ _i_), and minimize total cost."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "However, computing a least squares fit is quick, easy, and often good enough, so here’s how:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ol>\n<li>\n<p>Compute the sample means, x̄ and ȳ, the variance of <em>X</em>, and the covariance of <em>X</em> and <em>Y</em>.</p>\n</li>\n<li>\n<p>The estimated slope is</p>\n</li>\n<li>\n<p>And the intercept is</p>\n</li>\n\n</ol>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To see how this is derived, you can read [http://wikipedia.org/wiki/Numerical\\_methods\\_for\\_linear\\_least\\_squares](http://wikipedia.org/wiki/Numerical_methods_for_linear_least_squares)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Correlation and Causation"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In general, a relationship between two variables does not tell you whether one causes the other, or the other way around, or both, or whether they might both be caused by something else altogether (see [xkcd web comic](http://xkcd.com/552/))."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This rule can be summarized with the phrase “Correlation does not imply causation,” which is so pithy it has its own Wikipedia page: [http://wikipedia.org/wiki/Correlation\\_does\\_not\\_imply\\_causation](http://wikipedia.org/wiki/Correlation_does_not_imply_causation)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "So what can you do to provide evidence of causation?"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ol>\n<li>\n<p>Use time. If A comes before B, then A can cause B but not the other way around (at least according to our common understanding of causation). The order of events can help us infer the direction of causation, but it does not preclude the possibility that something else causes both A and B.</p>\n</li>\n<li>\n<p>Use randomness. If you divide a large population into two groups at random and compute the means of almost any variable, you expect the difference to be small. This is a consequence of the Central Limit Theorem (so it is subject to the same requirements).</p>\n</li>\n\n</ol>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If the groups are nearly identical in all variable but one, you can eliminate spurious relationships."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This works even if you don’t know what the relevant variables are, but it works even better if you do, because you can check that the groups are identical."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "These ideas are the motivation for the _randomized controlled trial_, in which subjects are assigned randomly to two (or more) groups: a _treatment_ group that receives some kind of intervention, like a new medicine, and a _control group_ that receives no intervention, or another treatment whose effects are known."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "A randomized controlled trial is the most reliable way to demonstrate a causal relationship, and the foundation of science-based medicine (see [http://wikipedia.org/wiki/Randomized\\_controlled\\_trial](http://wikipedia.org/wiki/Randomized_controlled_trial))."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Unfortunately, controlled trials are only possible in the laboratory sciences, medicine, and a few other disciplines. In the social sciences, controlled experiments are rare, usually because they are impossible or unethical."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "One alternative is to look for a _natural experiment_, where different “treatments” are applied to groups that are otherwise similar. One danger of natural experiments is that the groups might differ in ways that are not apparent. You can read more about this topic at [http://wikipedia.org/wiki/Natural\\_experiment](http://wikipedia.org/wiki/Natural_experiment)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In some cases it is possible to infer causal relationships using _regression analysis_. A linear least squares fit is a simple form of regression that explains a dependent variable using one explanatory variable. There are similar techniques that work with arbitrary numbers of independent variables."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "I won’t cover those techniques here, but there are also simple ways to control for spurious relationships. For example, in the NSFG, we saw that first babies tend to be lighter than others (see ???). But birth weight is also correlated with the mother’s age, and mothers of first babies tend to be younger than mothers of other babies."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "So it may be that first babies are lighter because their mothers are younger. To control for the effect of age, we could divide the mothers into age groups and compare birth weights for first babies and others in each age group."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If the difference between first babies and others is the same in each age group as it was in the pooled data, we conclude that the difference is not related to age. If there is no difference, we conclude that the effect is entirely due to age. Or, if the difference is smaller, we can quantify how much of the effect is due to age."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "."
        }
      ],
      "metadata": {
      }
    }
  ]
}